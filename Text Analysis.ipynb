{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work Flow\n",
    "\n",
    "## According to the listing ID (listing_ID) to match the listing reviews (use python or SQL), get the cluster category where it is located, and get three pieces of comment data according to the poly## category category (1,2,3)\n",
    "\n",
    "## (optional) Sampling from the review data according to the proportion of the three types of clustering results to obtain three sample review data\n",
    "\n",
    "## Filter English comment data and translate it into Chinese uniformly, keep Chinese comment data\n",
    "\n",
    "## Combine Chinese and English comment data\n",
    "\n",
    "## Import text analysis module\n",
    "\n",
    "## Word segmentation topic model results\n",
    "\n",
    "## Give business advice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call Baidu Translation Open Platform API module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-466eb8615667>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mappkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'kTb02y77pG3ZNQVW5RUG'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0msalt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32768\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m65536\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mREQUEST_FAILED\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;31m# 请求失败码\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "# Comment data English translation module (call Baidu translation open platform API)\n",
    "## Set api ID, API key\n",
    "## Set conversion language\n",
    "## Set the sentence to be translated quary\n",
    "## Set Baidu Universal Translation API Path\n",
    "## Generate a random number secret key, convert it into json format (encode function), and make URL encoding of the content (quary) after splicing.\n",
    "## Create a submission format header and convert the query content into url encoding format\n",
    "## Send an HTTP request and return the result in json format\n",
    "## Encode Python objects into JSON strings and output the translation results in json format.\n",
    "\n",
    "\n",
    "## API file name: baidu_API.py\n",
    "## Import necessary libraries\n",
    "import random\n",
    "from hashlib import md5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import json\n",
    "import hashlib\n",
    "import time, re\n",
    "\n",
    "## Set request conditions\n",
    "appid = '20210329000750760'\n",
    "appkey = 'kTb02y77pG3ZNQVW5RUG'\n",
    "encoding = 'utf-8'\n",
    "salt = str(random.randint(32768, 65536))\n",
    "REQUEST_FAILED = -1 # Request failure code\n",
    "\n",
    "\n",
    "## Define encryption function\n",
    "def getMD5(content):\n",
    "    m2 = hashlib.md5()\n",
    "    m2.update(content.encode(encoding))\n",
    "    return m2.hexdigest()\n",
    "\n",
    "## Define the request function\n",
    "def getTranslateResponce(url, data):\n",
    "    data = urllib.parse.urlencode(data).encode('utf-8')\n",
    "    response = urllib.request.urlopen(url, data)\n",
    "    return response.read().decode('utf-8')\n",
    "\n",
    "## Define the translation function and return the translation result\n",
    "def trans(content):\n",
    "    url = 'http://api.fanyi.baidu.com/api/trans/vip/translate'\n",
    "    data = {}\n",
    "    data['appid'] = appid\n",
    "    data['salt'] = salt\n",
    "    data['from'] = 'en'\n",
    "    data['to'] = 'zh'\n",
    "    data['q'] = content\n",
    "    data['sign'] = getMD5(appid + content + salt +appkey)\n",
    "    html = getTranslateResponce(url, data)\n",
    "    target = json.loads(html)\n",
    "    while target.get('error_code', REQUEST_FAILED) != REQUEST_FAILED:\n",
    "        time.sleep(1)\n",
    "        html = getTranslateResponce(url, data)\n",
    "        target = json.loads(html)\n",
    "    return target['trans_result'][0]['dst']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分析模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text analysis module\n",
    "\n",
    "## Import related libraries and Chinese filter word packs (thesaurus needs to be added）\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from collections import Counter\n",
    "string=open(r'C:\\Users\\le\\Desktop\\python项目代码汇总\\stopwords.txt','r',encoding='UTF-8').read()\n",
    "filterwords=string.split('\\n')\n",
    "\n",
    "##Ignore warning\n",
    "import logging\n",
    "import warnings\n",
    "logging.disable(logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## Function model package\n",
    "### Define the word segmentation function and store the word segmentation of each comment data in the list\n",
    "def word_cut(coms):\n",
    "    b=[]\n",
    "    for i in jieba.cut(coms):\n",
    "        if i not in filterwords:\n",
    "            b.append(i)\n",
    "    return b\n",
    "\n",
    "### Numericalize text comments (eg: [1,1,1,1,0,0.....])\n",
    "def get_vector(sentence,vocab):\n",
    "    temp=[]\n",
    "    for word in vocab:\n",
    "        if word in sentence:\n",
    "            temp.append(1)\n",
    "        else:\n",
    "            temp.append(0)\n",
    "    return temp\n",
    "\n",
    "### Package LDA model\n",
    "def get_lda(params):\n",
    "    corpora_words=[]\n",
    "    for i in params:\n",
    "        ss=word_cut(i)\n",
    "        corpora_words.append(ss)\n",
    "    words=[]\n",
    "    for i in corpora_words:\n",
    "        words+=i\n",
    "    word_count=Counter(words)\n",
    "    vocab=[]\n",
    "    for word in word_count.keys():\n",
    "        if word_count[word]>10:\n",
    "            ## Word frequency parameters are adjusted according to the situation\n",
    "            vocab.append(word)\n",
    "    X=[]\n",
    "    for se in corpora_words:\n",
    "        X.append(get_vector(se,vocab))\n",
    "    X=np.array(X)\n",
    "    lda = LatentDirichletAllocation(n_topics=5,\n",
    "                                learning_offset=50,\n",
    "                                random_state=0)\n",
    "   ## Build the number of topics to adjust according to the situation\n",
    "    lda.fit(X)\n",
    "    for i in range(5):\n",
    "        index=np.argsort(lda.components_[i])[::-1]\n",
    "        ##After sorting in descending order, find the corresponding index of the corresponding word under each topic on the vocab table\n",
    "        print('主题',i,':',end='')\n",
    "        for j in np.array(vocab)[index][:20]:\n",
    "            ## According to the index, find out the top n words of each topic in descending order according to the model index\n",
    "            print(j,end=' ')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine whether the string contains English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "包含英文字符\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Determine whether it contains English\n",
    "str = \"1!!!a\"\n",
    "contain_en = bool(re.search('[a-z]', str.lower()))\n",
    "\n",
    "if contain_en:\n",
    "    print(\"包含英文字符\")\n",
    "else:\n",
    "    print(\"不包含英文字符\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-7c2e67e80b1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mcomment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0moriginal_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mtranslation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mtranslation_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtranslation_sheet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'原文'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0moriginal_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'译文'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtranslation_list\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "# Get the translated comment data and save it in the airbnb_Selection_comment table\n",
    "\n",
    "df=pd.read_excel(r'C:\\Users\\le\\Desktop\\Business Data Analysis\\爱彼迎英文评论测试.xlsx')\n",
    "row = df.shape[0]  # 获得行数\n",
    "col = df.shape[1]  # 获得列数\n",
    "original_list = []\n",
    "translation_list =[]\n",
    "from baidu_API import *\n",
    "## Extract all comment text, save the text before and after translation in a different list, and save it as excel\n",
    "for c in range(1,col):\n",
    "    for r in range(0,row):\n",
    "        comment = df.iloc[r][c]\n",
    "        original_list.append(comment)\n",
    "        translation = str(trans(str(commentP).replace('\\n','')))  \n",
    "        translation_list.append(translation)\n",
    "        translation_sheet=pd.DataFrame({'原文':original_list,'译文':translation_list})\n",
    "        translation_sheet.to_csv('C:/Users/le/Desktop/Business Data Analysis/airbnb_Translation_comment.csv',encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the first and third types of sampled review data and the second type of overall review data\n",
    "# Adjust model parameters\n",
    "# Analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model module\n",
    "# Text analysis module\n",
    "\n",
    "## Import related libraries and Chinese filter word packs (thesaurus needs to be added)\n",
    "import jieba\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from collections import Counter\n",
    "string=open(r'C:\\Users\\le\\Desktop\\python项目代码汇总\\stopwords.txt','r',encoding='UTF-8').read()\n",
    "punctuation='[’!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n。！，“”]+ ｡ ､ ｡; …  ｡'\n",
    "filterwords=string.split('\\n')\n",
    "\n",
    "##Ignore warning\n",
    "import logging\n",
    "import warnings\n",
    "logging.disable(logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## Function model package\n",
    "### Define the word segmentation function and store the word segmentation of each comment data in the list\n",
    "def word_cut(coms):\n",
    "    b=[]\n",
    "    for i in jieba.cut(coms):\n",
    "        if i not in filterwords:\n",
    "            i=re.sub(punctuation,'',i) ## 过滤标点\n",
    "            i=re.sub('\\n','',i)\n",
    "            i=re.sub('[\\d]','',i)## 过滤数字\n",
    "            b.append(i)\n",
    "    return b\n",
    "\n",
    "### Numericalize text comments (eg: [1,1,1,1,0,0.....])\n",
    "def get_vector(sentence,vocab):\n",
    "    temp=[]\n",
    "    for word in vocab:\n",
    "        if word in sentence:\n",
    "            temp.append(1)\n",
    "        else:\n",
    "            temp.append(0)\n",
    "    return temp\n",
    "\n",
    "\n",
    "### 封装LDA模型\n",
    "def get_lda(params):\n",
    "    corpora_words=[]\n",
    "    for i in params:\n",
    "        ss=word_cut(i)\n",
    "        corpora_words.append(ss)\n",
    "    words=[]\n",
    "    for i in corpora_words:\n",
    "        words+=i\n",
    "    word_count=Counter(words)\n",
    "    vocab=[]\n",
    "    for word in word_count.keys():\n",
    "        if word_count[word]>100:\n",
    "            ## Word frequency parameters are adjusted according to the situation\n",
    "            vocab.append(word)\n",
    "    X=[]\n",
    "    for se in corpora_words:\n",
    "        X.append(get_vector(se,vocab))\n",
    "    X=np.array(X)\n",
    "    lda = LatentDirichletAllocation(n_topics=5,\n",
    "                                learning_offset=50,\n",
    "                                random_state=0)\n",
    "  ## Build the number of topics to adjust according to the situation\n",
    "    lda.fit(X)\n",
    "    for i in range(5):\n",
    "        index=np.argsort(lda.components_[i])[::-1]\n",
    "        ##After sorting in descending order, find the corresponding index of the corresponding word under each topic on the vocab table\n",
    "        print('主题',i,':',end='')\n",
    "        for j in np.array(vocab)[index][:20]:\n",
    "            ## According to the index, find the top n words of each topic in descending order according to the model index\n",
    "            print(j,end=' ')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一类（质量好的优质房源评论）-----超赞房东"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv(r'C:/Users/le/Desktop/Business Data Analysis/segment1_sample.csv',encoding='utf-8') # 共计3000条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题 0 :干净 交通 ｡ 整洁 便利 齐全 设施 位置 环境 安静 温馨 满意 适合 热情 舒适 小区 推荐 住宿 装修 ~ \n",
      "主题 1 :挺 房主 阿姨 棒 地铁 可爱 体验 吃 热情  楼下 生活 ~ 很近 家 东西 推荐 晚上 贴心 设施 \n",
      "主题 2 :｡ 地铁站  位置 近 回复 干净 地理位置 很近 距离 步行 小区 入住 出行 地铁 一个 走 分钟 热情 装修 \n",
      "主题 3 :｡ 体验 入住 北京 性价比 住 下次 高 推荐 真的 舒适 民宿 感觉 干净 胡同 位置 很多 热情 ~ 还会 \n",
      "主题 4 :舒服 住 特别 ｡ 超级 小姐姐 ~ 温馨 地方 热情 干净 下次 家里 床 真的 北京 开心 可爱 感觉 近 \n"
     ]
    }
   ],
   "source": [
    "list_comment=df2['comments']\n",
    "get_lda(list_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三类（质量好的房源评论）-----非超赞房东"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=pd.read_csv(r'C:/Users/le/Desktop/Business Data Analysis/segment3_sample.csv',encoding='utf-8')  # 共计1500条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题 0 :｡ 热情 推荐 设施 地理位置 特别 齐全 干净 温馨 性价比 很多 位置 整洁 入住 ~ 挺 下次 ､ 北京 住 \n",
      "主题 1 :位置 ｡  小区 挺 一个 地铁站 干净 性价比 住 ､ 很多 ~ 整洁 入住 齐全 体验 设施 北京 热情 \n",
      "主题 2 :｡ 住 舒服 北京 下次 近 感觉 干净 特别 地铁站 热情 很多  温馨 ､ 性价比 挺 整洁 位置 ~ \n",
      "主题 3 :交通 便利 ｡ 干净 热情 地铁站 位置 齐全 е о а н т 近 北京 整洁 温馨 下次 住 小区 \n",
      "主题 4 :体验 入住 ｡ 干净 ~ 舒适 整洁 热情 住 温馨 位置 性价比 下次 地铁站 挺 近 齐全  设施 舒服 \n"
     ]
    }
   ],
   "source": [
    "list_comment=df3['comments']\n",
    "get_lda(list_comment)\n",
    "# 参数说明：主题数：5 每个主题显示20个词   高频词定义标准：累计出现100次\n",
    "\n",
    "# 结论分析：\n",
    "# 1 入住体验类：年轻女性类的房东的的亲和力较高，给租客传达出了一种温馨热情的入住体验\n",
    "# 2 地理位置类：距离地铁站距离近，地理位置优越的房子，提供了更为便利的交通\n",
    "# 3 房屋配置类：设施配置齐全，房屋环境干净，令租客感觉性价比高\n",
    "\n",
    "# Business Insight：\n",
    "# 1  筛选优质评论中含有关键词的评论，匹配其对应的优质房源，撰写优质干净房源文案，以滑动banner位的形式上传到APP首页，\n",
    "# 给平台上北京地区的其他房东提供改善房源的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二类（质量差的房源评论）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(r'C:/Users/le/Desktop/Business Data Analysis/segment2.csv',encoding='utf-8')  # 共计584条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题 0 :自动 主人 取消 预订 这是 投递 天 抵达 到达  前一天 前三天 前两天 估计 ｡ 设施 ; 位置 数据 点 \n",
      "主题 1 :前两天 到达 这是 主人 投递 取消 自动 预订 前一天 一个 照片 地方 电话 回复 晚上 找 很多 房源 走 干净 \n",
      "主题 2 :投递 取消 预订 这是 自动 主人 天  到达 抵达 房源 估计 ｡ 地方 购买 设施 住 前一天 地区 没 \n",
      "主题 3 :｡  入住 地方 一个 干净 体验 卫生间 没 住 差 房源 找 真的 晚上 空调 卫生 厨房 设施 发现 \n",
      "主题 4 :住 ｡ 图片 没 睡 不好 一个 床 房主 回复 电话 晚上 房源 点 走 真的 取消 坏 两个 差 \n"
     ]
    }
   ],
   "source": [
    "list_comment=df1['comments']\n",
    "get_lda(list_comment)\n",
    "# 参数说明：主题数：5 每个主题显示20个词   高频词定义标准：累计出现10次\n",
    "\n",
    "# 结论分析：\n",
    "# 1 取消预订类： 房东在租客临近入住第1,2,3天时，由于房东取消订单导致系统自动发布不良评价\n",
    "# 2 硬件设施类：租客反应厨房和卫生间里得硬件因为清洁问题影响用户的入住体验\n",
    "\n",
    "# Business Insight：\n",
    "# 1 房东侧：在接受订单时设置惩罚提醒 。\n",
    "# 2 在房东平均分计算体系中加入厨房和卫生间的评级维度，关注厨房和卫生间的权重比例。\n",
    "# 3 将有厨房和卫生间有负面反馈的房东发送短信push，促使其通过改善这两者的卫生条件来提升其平均分。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
